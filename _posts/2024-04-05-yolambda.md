---
title: "#TGIFunüéà YOLambda: Running Serverless YOLOv8/9"
layout: default
excerpt_separator: <!-- excerpt-end -->
---

## \#TGIFunüéà YOLambda: Running Serverless YOLOv8/9

### Overview

<!-- excerpt-start -->

In this episode of #TGIFunüéà, I'd like to demonstrate a quick and easy way to deploy YOLOv8/9 üëÅÔ∏è on [AWS Lambda](https://aws.amazon.com/lambda/) using the [AWS SAM](https://aws.amazon.com/serverless/sam/) (Serverless Application Model) CLI.

<!-- excerpt-end -->

Hosting YOLO on Lambda strikes a good balance between performance, scalability and cost efficiency. Plus, it's always fun to put stuff inside Lambda functions. If you're interested in exploring other deployment options though, feel free to scroll all the way down to the References section.

> üë®‚Äçüíª All code and documentation is available at [github.com/JGalego/YOLambda](https://github.com/JGalego/YOLambda).

### YOLO in Pictures üñºÔ∏è

So what's YOLOv8 and why should you care? Let's start with a short recap...

[YOLOv8](https://yolov8.com/) (*You Only Look Once*) is a state-of-the-art computer vision model that supports multiple tasks.

<div align="center">
<img src="/assets/images/yolo_tasks.png" width="75%" alt="Source: Ultralytics (https://docs.ultralytics.com/tasks/)"/>
</div>

![](assets/images/yolo_tasks.png)

It builds on top of an already long [history of YOLO models](https://deci.ai/blog/history-yolo-object-detection-models-from-yolov1-yolov8/)

<div align="center">
<img src="/assets/images/yolo_history.png" width="75%" alt="Source: Ali & Zhang (2024)"/>
</div>

and it was designed to be smaller ü§è and faster ‚ö° than previous iterations.

<div align="center">
<img width="75%" src="https://raw.githubusercontent.com/ultralytics/assets/refs/heads/main/yolo/performance-comparison.png" alt="Source: Ultralytics (https://github.com/ultralytics/ultralytics)">
</div>

While a full description of the YOLOv8+ architecture is well beyond the scope of this article, it's useful to gain some intuition on what's happening behind the scenes.

Referring back to the original ([YOLOv1](https://arxiv.org/abs/2304.00501)) paper, YOLO models work by dividing the input image into a grid, predicting a set of [bounding boxes](https://www.ayadata.ai/blog-posts/bounding-boxes-in-computer-vision-uses-best-practices-for-labeling-and-more/), which are expressed as 2+2-tuples of top-left `(x1, y1)` and bottom-right coordinates `(x2, y2)`, as well as their associated confidence scores and class probabilities, to generate the final predictions.

<div align="center">
<img width="75%" src="/assets/images/bounding_boxes+detections.png" alt="Source: Redmon et al. (2015)">
</div>

It goes without saying that I'm obviously oversimplifying things here.

Over the years, there have been many improvements like faster NMS implementations (which stands for [Non-Maximum Suppression](https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch/), in case you're wondering) or the use of "bag-of-freebies" and "bag-of-specials" approaches (*best names ever!*) that have made YOLO faster and stronger.

Fortunately, you won't need to care about those at all to work on this project. If you're interested in such things though, I strongly encourage you to read up on the [history of YOLO](https://deci.ai/blog/history-yolo-object-detection-models-from-yolov1-yolov8/).

>‚ùó This introduction was written before the release of [YOLOv9](https://docs.ultralytics.com/models/yolov9/).

> üí° If you want to learn more, just scroll all the way down to the **References** section.

### Goal üéØ

In this project, we're going to create a simple object detection app that accepts an image üñºÔ∏è

<div align="center">
<img width="75%" src="/assets/images/example.jpg" alt="And it all started with a Big Bang...">
</div>

sends it to the YOLO model and returns a list of detected objects

```json
[
    {
        "box": [
            962,
            154,
            1247,
            630
        ],
        "conf": 0.8881231546401978,
        "cls": "person"
    },
    {
        "box": [
            67,
            227,
            502,
            711
        ],
        "conf": 0.8832821846008301,
        "cls": "person"
    },
    {
        "box": [
            5,
            45,
            240,
            578
        ],
        "conf": 0.8401730060577393,
        "cls": "person"
    },
    {
        "box": [
            733,
            88,
            936,
            460
        ],
        "conf": 0.809768795967102,
        "cls": "person"
    },
    {
        "box": [
            308,
            98,
            556,
            442
        ],
        "conf": 0.7752255201339722,
        "cls": "person"
    },
    {
        "box": [
            903,
            2,
            1088,
            505
        ],
        "conf": 0.7346365451812744,
        "cls": "person"
    },
    {
        "box": [
            534,
            149,
            769,
            391
        ],
        "conf": 0.6235901117324829,
        "cls": "person"
    },
    {
        "box": [
            632,
            338,
            672,
            467
        ],
        "conf": 0.40179234743118286,
        "cls": "bottle"
    },
    {
        "box": [
            552,
            387,
            614,
            467
        ],
        "conf": 0.617901086807251,
        "cls": "cup"
    },
    {
        "box": [
            1101,
            376,
            1279,
            639
        ],
        "conf": 0.3513599634170532,
        "cls": "couch"
    },
    {
        "box": [
            426,
            364,
            915,
            614
        ],
        "conf": 0.46763089299201965,
        "cls": "dining table"
    }
]
```

which we can then place on top of the original image

<div align="center">
<img width="75%" src="/assets/images/output_yolo.png" alt="BANG!">
</div>

We're going to use a vanilla YOLOv8 model, but you're more than welcome to use a [fine-tuned model](https://universe.roboflow.com/search?q=model:yolov8) or to [train your own YOLO](https://blog.roboflow.com/how-to-train-yolov8-on-a-custom-dataset/).

<div align="center">
<img width="75%" src="/assets/images/yolov8_fine_tuned_models.png" alt="Source: Roboflow Universe (https://universe.roboflow.com/search?q=model:yolov8)"/>
</div>

**Sounds fun?** ü§© Then buckle up and let's build it together!

### Instructions

#### Prerequisites ‚úÖ

Before we get started, make sure these tools are installed and properly configured:

- üêç [Conda](https://conda.io/projects/conda/en/latest/user-guide/install/index.html) (preferred) or vanilla [Python](https://www.python.org/) (version `>=3.9`)
- üê≥ [Docker](https://docs.docker.com/engine/install/)
- üêøÔ∏è [AWS SAM](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/install-sam-cli.html)
- (Optional) [JQ](https://jqlang.github.io/jq/) for JSON processing in the terminal

```bash
# Python / Conda
python -V
conda info

# Docker
docker info

# AWS SAM
sam --info

# JQ
jq -h
```

#### Steps üìú

Let's start by cloning the repository

```bash
git clone https://github.com/JGalego/YOLambda
cd YOLambda
```

> üß™ **Experimental:** Switch to the [`feat/YOLOv9`](https://github.com/JGalego/YOLambda/tree/feat/YOLOv9) branch if you dare!

As a best practice, I recommend you create a Conda environment, virtualenv or something similar to keep everything isolated

```bash
# Create a new environment and activate it
conda env create -f environment.yml
conda activate yolambda
```

Once the environment is activated, we can kick things off and install the project dependencies

```bash
pip install -r requirements.txt
```

One of those dependencies is the [`ultralytics`](https://github.com/ultralytics/ultralytics) package which includes the `yolo` CLI, which we can use to download the YOLOv8 model and convert it to the [ONNX](https://onnx.ai/) format:

> üí° The YOLOv8 series offers a wide range of models both in terms of size (`n`ano >> `x`l) and specialized task like `seg`mentation or `pose` estimation. If you want to try a different model, please refer to the official documentation ([Supported Tasks and Modes](https://docs.ultralytics.com/models/yolov8/#supported-tasks-and-modes)).

> üß™ **Experimental:** Replace `yolov8n` with `yolov9c` in the commands below to work with YOLOv9. Just keep in mind that the performance and the output of our app may not be the same.

```bash
# Export nano model for object detection from PT to ONNX
yolo mode=export model=yolov8n.pt format=onnx dynamic=True
```

As an optional step, we can run [ONNX Simplifier](https://github.com/daquexian/onnx-simplifier) (based on [ONNX Optimizer](https://github.com/onnx/optimizer)) against our model to get rid of redundant operations. Everything counts to reduce the size of our model and make it run faster.

```bash
# (Optional) Simplify
# https://github.com/daquexian/onnx-simplifier
onnxsim yolov8n.onnx yolov8n.onnx
```

<div align="center">
<img width="30%" src="/assets/images/onnx_simplify.png"/>
</div>

Once this is done, we can look "under the hood" and take a peek at the computational graph with a tool like [Netron](https://netron.app/):

```bash
# üåê Browser
# Visit https://netron.app/

# üíª CLI
# https://github.com/lutzroeder/netron
netron -b yolov8n.onnx
```

<div align="center">
<img width="75%" src="assets/images/yolo_computational_graph_detail.png"/>
</div>

Let's move our model to a dedicated folder

```bash
mkdir models; mv yolov8n.onnx $_
```

and use the [AWS SAM](https://aws.amazon.com/serverless/sam/) CLI to build a container image for our app

```bash
# Build
sam build --use-container
```

While in development, we can test our app by using `sam local`

```bash
# Create event
echo {\"body\": \"{\\\"image\\\": \\\"$(base64 images/example.jpg)\\\"}\"} > test/event.json

# Invoke function
sam local invoke --event test/event.json
```

Whenever you're ready, just start the deployment üöÄ

```bash
# Deploy
sam deploy --guided
```

While this is running, let's take a closer look at our template:

```yaml
AWSTemplateFormatVersion: '2010-09-09'
Transform: AWS::Serverless-2016-10-31
Description: >
  Scaling YOLOv8 inference with Serverless:
  How to build an object detection app using AWS Lambda and AWS SAM

Resources:
  YOLOModel:
    Type: AWS::Serverless::LayerVersion
    Properties:
      LayerName: yolo-models
      Description: YOLO models
      ContentUri: models/
      CompatibleRuntimes:
        - python3.9
        - python3.10
        - python3.11
  YOLOFunction:
    Type: AWS::Serverless::Function
    Properties:
      CodeUri: src/
      Layers:
        - !Ref YOLOModel
      Handler: app.handler
      Runtime: python3.10
      MemorySize: 10240
      Timeout: 60
      FunctionUrlConfig:
        AuthType: AWS_IAM

Outputs:
  YOLOV8FunctionUrlEndpoint:
      Description: "YOLO Lambda function URL"
      Value:
        Fn::GetAtt: YOLOFunctionUrl.FunctionUrl
```

Here are a few important things to notice:

- üß± **Resources** - there's one for the [Lambda function](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-resource-function.html) (`YOLOFunction`) and another one for the YOLOv8 model (`YOLOModel`) which will be added to our function as a [Lambda layer](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-resource-layerversion.html); the Lambda function itself will be accessible through a [Lambda function URL](https://docs.aws.amazon.com/lambda/latest/dg/lambda-urls.html).

- ‚öôÔ∏è **Settings** - the memory size is set to the maximum allowed value (`10GB`) to improve performance cf. [AWS Lambda now supports up to 10 GB of memory and 6 vCPU cores for Lambda Functions](https://aws.amazon.com/about-aws/whats-new/2020/12/aws-lambda-supports-10gb-memory-6-vcpu-cores-lambda-functions/) for more information.

- üîê **Security** - authentication to our function URL is handled by IAM, which means that all requests must be signed using [AWS Signature Version 4](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_aws-signing.html) (SigV4) cf. [Invoking Lambda function URLs](https://docs.aws.amazon.com/lambda/latest/dg/urls-invocation.html) for additional details.

The deployment should be done by the time if you finish reading the text above üòä Don't forget to note down the function URL.

```bash
# Requires jq
export YOLAMBDA_URL=$(sam list stack-outputs --stack-name yolambda --output json | jq -r .[0].OutputValue)
echo $YOLAMBDA_URL
```

You can use tools like awscurl  to test the app (awscurl will handle the SigV4 signing for you)

```bash
# Create payload
echo {\"image\": \"$(base64 images/example.jpg)\"} > test/payload.json

# Make request
awscurl --service lambda -X GET -d @test/payload.json $YOLAMBDA_URL
```

or create your own test scripts

```bash
python test/test.py $YOLAMBDA_URL images/example.jpg
```

<div align="center">
<img width="75%" src="/assets/images/yolambda_pipeline.png"/>
</div>

And that's it! ü•≥ We just crossed the finish line...

Sooo, what's next? Here are a few recommendations:

- **Explore the code** - it's just there for the taking, plus I left some Easter eggs and L400 references in there for the brave ones.
   - **Check out the feat/YOLOv9 branch** to test the newest member of the YOLO family
- **Build your own app** - I'm pretty sure you already have a cool use case in mind
- **Share with the community** - leave a comment below if you do something awesome

I hope you enjoyed it, see you next time! üëã

> This was the first article in the #TGIFunüéà series, a personal space where I'll be sharing some small, hobby-oriented projects with a wide variety of applications, I used to publish on the AWS Community Blog. AWS messed up the formatting on the original, so I decided to move it here. Enjoy!

### References üìö

#### Articles

- (Redmon *et al.*, 2015) [You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640)
- (Terven & Cordova-Esparza, 2023) [A Comprehensive Review of YOLO Architectures in Computer Vision: From YOLOv1 to YOLOv8 and YOLO-NAS](https://arxiv.org/abs/2304.00501)

#### Blogs

- [Hosting YOLOv8 PyTorch models on Amazon SageMaker Endpoints](https://aws.amazon.com/blogs/machine-learning/hosting-yolov8-pytorch-model-on-amazon-sagemaker-endpoints/)
- [Intuitivo achieves higher throughput while saving on AI/ML costs using AWS Inferentia and PyTorch](https://aws.amazon.com/blogs/machine-learning/intuitivo-achieves-higher-throughput-while-saving-on-ai-ml-costs-using-aws-inferentia-and-pytorch/)
- [Scale YOLOv5 inference with Amazon SageMaker endpoints and AWS Lambda](https://aws.amazon.com/blogs/machine-learning/scale-yolov5-inference-with-amazon-sagemaker-endpoints-and-aws-lambda/)
- [Speed up YOLOv4 inference to twice as fast on Amazon SageMaker](https://aws.amazon.com/blogs/machine-learning/speed-up-yolov4-inference-to-twice-as-fast-on-amazon-sagemaker/)
- [Run computer vision inference on large videos with Amazon SageMaker asynchronous endpoints](https://aws.amazon.com/blogs/machine-learning/run-computer-vision-inference-on-large-videos-with-amazon-sagemaker-asynchronous-endpoints/)
- [Achieving 1.85x higher performance for deep learning based object detection with an AWS Neuron compiled YOLOv4 model on AWS Inferentia](https://aws.amazon.com/blogs/machine-learning/improving-performance-for-deep-learning-based-object-detection-with-an-aws-neuron-compiled-yolov4-model-on-aws-inferentia/)
- [Streamlining data labeling for YOLO object detection in Amazon SageMaker Ground Truth](https://aws.amazon.com/blogs/machine-learning/streamlining-data-labeling-for-yolo-object-detection-in-amazon-sagemaker-ground-truth/)

#### Miscellaneous

- [Ultralytics YOLOv8 Docs](https://docs.ultralytics.com/)
  - [Ultralytics YOLOv8 Tasks](https://docs.ultralytics.com/tasks/)
  - [YOLOv9: A Leap Forward in Object Detection Technology üß™](https://docs.ultralytics.com/models/yolov9/)
  - [YOLOv5 üöÄ on AWS Deep Learning Instance: Your Complete Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)
- [LearnOpenCV](https://learnopencv.com/)
  - [Non Maximum Suppression: Theory and Implementation in PyTorch](https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch/)
- [HowTo: deploying YOLOv8 on AWS Lambda  (an alternative implementation üí™](https://www.trainyolo.com/blog/deploy-yolov8-on-aws-lambda)